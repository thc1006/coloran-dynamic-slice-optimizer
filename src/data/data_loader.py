# src/data/data_loader.py

import os
import subprocess
import pandas as pd
import glob
import warnings

warnings.filterwarnings('ignore')

class ColoRANDataLoader:
    """
    ColO-RAN è³‡æ–™é›†è¼‰å…¥å™¨ã€‚
    è² è²¬å¾é ç«¯å„²å­˜åº«ä¸‹è¼‰è³‡æ–™é›†ï¼Œä¸¦å°‡åŸå§‹ CSV æª”æ¡ˆè¼‰å…¥åˆ° pandas DataFrame ä¸­ã€‚
    """
    def __init__(self, base_path="/content"):
        self.dataset_repo_url = "https://github.com/wineslab/colosseum-oran-coloran-dataset.git"
        self.dataset_local_path = os.path.join(base_path, "colosseum-oran-coloran-dataset")
        self.base_stations = [1, 8, 15, 22, 29, 36, 43]
        self.scheduling_policies = ['sched0', 'sched1', 'sched2'] # RR, WF, PF
        self.training_configs = [f'tr{i}' for i in range(28)]
        print(f"ğŸš€ åˆå§‹åŒ– ColoRANDataLoaderï¼Œè³‡æ–™è·¯å¾‘è¨­å®šç‚º: {self.dataset_local_path}")

    def download_dataset(self):
        """ä¸‹è¼‰ ColO-RAN è³‡æ–™é›†ï¼Œå¦‚æœæœ¬åœ°ä¸å­˜åœ¨ã€‚"""
        print("æª¢æŸ¥ ColO-RAN è³‡æ–™é›†...")
        if os.path.exists(self.dataset_local_path):
            print(f"âœ… è³‡æ–™å¤¾ '{self.dataset_local_path}' å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰ã€‚")
            return self.dataset_local_path

        print("è³‡æ–™é›†ä¸å­˜åœ¨ï¼Œé–‹å§‹ä¸‹è¼‰...")
        try:
            result = subprocess.run(
                ["git", "clone", self.dataset_repo_url, self.dataset_local_path],
                capture_output=True, text=True, timeout=600, check=True
            )
            print("âœ… è³‡æ–™é›†ä¸‹è¼‰æˆåŠŸï¼")
            print("\nğŸ“ è³‡æ–™é›†çµæ§‹é è¦½ï¼š")
            os.system(f"ls -la {self.dataset_local_path}")
            return self.dataset_local_path
        except subprocess.TimeoutExpired:
            print("âŒ ä¸‹è¼‰è¶…æ™‚ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·šã€‚")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Git clone å¤±æ•—: {e.stderr}")
        except Exception as e:
            print(f"âŒ ä¸‹è¼‰éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

    def _auto_detect_structure(self):
        """è‡ªå‹•åµæ¸¬æœ‰æ•ˆçš„è³‡æ–™é›†çµæ§‹è·¯å¾‘ã€‚"""
        possible_paths = [
            os.path.join(self.dataset_local_path, "rome_static_medium"),
            self.dataset_local_path,
        ]
        for path in possible_paths:
            if os.path.exists(path) and any(d.startswith('sched') for d in os.listdir(path)):
                print(f"âœ… æ‰¾åˆ°æœ‰æ•ˆè³‡æ–™çµæ§‹: {path}")
                return path
        print("âŒ æœªæ‰¾åˆ°æ¨™æº–è³‡æ–™çµæ§‹ã€‚")
        return None

    def load_raw_data(self):
        """ä½¿ç”¨ glob è¼‰å…¥æ‰€æœ‰åŸå§‹è³‡æ–™ã€‚"""
        base_data_path = self._auto_detect_structure()
        if not base_data_path:
            return None, None, None

        slice_data_list = []
        total_combinations = len(self.scheduling_policies) * len(self.training_configs) * len(self.base_stations)
        print(f"\nğŸš€ é–‹å§‹è¼‰å…¥å®Œæ•´ ColO-RAN è³‡æ–™é›†ï¼ˆ{total_combinations} å€‹çµ„åˆï¼‰")

        for sched_policy in self.scheduling_policies:
            for training_config in self.training_configs:
                search_pattern = f"{base_data_path}/{sched_policy}/{training_config}/exp*/bs*/slices_bs*/*_metrics.csv"
                slice_files = glob.glob(search_pattern)
                for slice_file in slice_files:
                    try:
                        df = pd.read_csv(slice_file)
                        path_parts = slice_file.split('/')
                        exp_folder = next((p for p in path_parts if p.startswith('exp')), 'exp1')
                        bs_folder = next((p for p in path_parts if p.startswith('bs') and 'slices' not in p), 'bs1')
                        df['bs_id'] = int(bs_folder.replace('bs', ''))
                        df['exp_id'] = int(exp_folder.replace('exp', ''))
                        df['imsi'] = os.path.basename(slice_file).replace('_metrics.csv', '')
                        df['sched_policy'] = sched_policy
                        df['training_config'] = training_config
                        slice_data_list.append(df)
                    except Exception as e:
                        print(f" âŒ Slice æª”æ¡ˆè¼‰å…¥å¤±æ•— {slice_file}: {e}")

        if not slice_data_list:
            print("âŒ æœªèƒ½è¼‰å…¥ä»»ä½•åˆ‡ç‰‡è³‡æ–™ã€‚")
            return None

        print("\nğŸ”— åˆä½µè³‡æ–™ä¸­...")
        combined_slice_data = pd.concat(slice_data_list, ignore_index=True)
        memory_mb = combined_slice_data.memory_usage(deep=True).sum() / 1024 / 1024
        print(f"âœ… åˆ‡ç‰‡è³‡æ–™è¼‰å…¥å®Œæˆ: {len(combined_slice_data):,} ç­†è¨˜éŒ„, {memory_mb:.1f} MB")
        return combined_slice_data

if __name__ == '__main__':
    # ç¯„ä¾‹ä½¿ç”¨
    data_loader = ColoRANDataLoader()
    data_loader.download_dataset()
    slice_data = data_loader.load_raw_data()
    if slice_data is not None:
        print("\nè³‡æ–™è¼‰å…¥å™¨æ¸¬è©¦æˆåŠŸï¼Œå·²è¼‰å…¥åˆ‡ç‰‡è³‡æ–™ã€‚")
        print(slice_data.head())

