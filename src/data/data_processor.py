# src/data/data_processor.py

import pandas as pd
import numpy as np
import gc
import warnings

warnings.filterwarnings('ignore')

class MemoryOptimizedProcessor:
    """
    å°è¼‰å…¥çš„ç¶²è·¯åˆ‡ç‰‡è³‡æ–™é€²è¡Œè¨˜æ†¶é«”å„ªåŒ–çš„ç‰¹å¾µå·¥ç¨‹ã€‚
    - å»ºç«‹æ™‚é–“ã€è³‡æºã€QoS ç­‰å¤šç¶­åº¦ç‰¹å¾µã€‚
    - æ¡ç”¨åˆ†æ‰¹è™•ç†ä»¥æ‡‰å°å¤§è¦æ¨¡è³‡æ–™é›†ã€‚
    - æœ€ä½³åŒ–è³‡æ–™å‹åˆ¥ä»¥é™ä½è¨˜æ†¶é«”ä½”ç”¨ã€‚
    """
    def __init__(self, slice_configs, batch_size=75000):
        self.slice_configs = slice_configs
        self.batch_size = batch_size
        self.feature_columns = [
            'num_ues', 'slice_id', 'sched_policy_num', 'allocated_rbgs',
            'bs_id', 'exp_id', 'sum_requested_prbs', 'sum_granted_prbs',
            'prb_utilization', 'throughput_efficiency', 'qos_score',
            'network_load', 'hour', 'minute', 'day_of_week'
        ]
        print(f"ğŸ”§ åˆå§‹åŒ–è¨˜æ†¶é«”å„ªåŒ–è™•ç†å™¨ï¼Œæ‰¹æ¬¡å¤§å°: {self.batch_size:,}")

    def _process_single_batch(self, df):
        """è™•ç†å–®ä¸€æ‰¹æ¬¡çš„è³‡æ–™ï¼Œå»ºç«‹æ‰€æœ‰å·¥ç¨‹ç‰¹å¾µã€‚"""
        # 1. æ™‚é–“ç‰¹å¾µ
        timestamps = pd.to_datetime(df['Timestamp'], unit='ms', errors='coerce')
        df['hour'] = timestamps.dt.hour
        df['minute'] = timestamps.dt.minute
        df['day_of_week'] = timestamps.dt.dayofweek

        # 2. æ’ç¨‹ç­–ç•¥ç·¨ç¢¼
        df['sched_policy_num'] = df['sched_policy'].map({'sched0': 0, 'sched1': 1, 'sched2': 2})

        # 3. RBG é…ç½®
        config_map = {(cfg, i): rbg for cfg, rbgs in self.slice_configs.items() for i, rbg in enumerate(rbgs)}
        df['allocated_rbgs'] = pd.Series(zip(df['training_config'], df.get('slice_id', 0))).map(config_map).fillna(0)

        # 4. è³‡æºåˆ©ç”¨ç‡
        df['prb_utilization'] = np.where(df['sum_requested_prbs'] > 0, df['sum_granted_prbs'] / df['sum_requested_prbs'], 0).clip(0, 1)

        # 5. ååé‡æ•ˆç‡
        df['throughput_efficiency'] = np.where(df['sum_granted_prbs'] > 0, df['tx_brate downlink [Mbps]'].fillna(0) / df['sum_granted_prbs'], 0)

        # 6. QoS è©•åˆ†
        dl_score = (100 - df['tx_errors downlink (%)'].fillna(50)) / 100
        ul_score = (100 - df['rx_errors uplink (%)'].fillna(50)) / 100
        cqi_score = df['dl_cqi'].fillna(7.5) / 15
        df['qos_score'] = (0.4 * dl_score + 0.3 * ul_score + 0.3 * cqi_score).clip(0, 1)

        # 7. ç¶²è·¯è² è¼‰
        df['network_load'] = df.get('num_ues', 1).fillna(1) / 42.0

        # 8. ç¶œåˆæ•ˆç‡æŒ‡æ¨™ (ç›®æ¨™è®Šæ•¸)
        df['allocation_efficiency'] = (0.5 * df['throughput_efficiency'] + 0.3 * df['qos_score'] + 0.2 * df['prb_utilization']).clip(0, 1)

        return df[self.feature_columns + ['allocation_efficiency']].dropna(subset=['allocation_efficiency'])

    def process_data(self, slice_data):
        """å°å®Œæ•´è³‡æ–™é›†é€²è¡Œåˆ†æ‰¹è™•ç†å’Œç‰¹å¾µå·¥ç¨‹ã€‚"""
        print(f"ğŸš€ é–‹å§‹åˆ†æ‰¹è™•ç†ï¼Œç¸½è¨˜éŒ„æ•¸: {len(slice_data):,}")
        total_rows = len(slice_data)
        num_batches = (total_rows + self.batch_size - 1) // self.batch_size
        
        processed_batches = []
        for i in range(num_batches):
            print(f" ğŸ“¦ è™•ç†æ‰¹æ¬¡ {i + 1}/{num_batches}...")
            batch = slice_data.iloc[i * self.batch_size:(i + 1) * self.batch_size]
            processed_batches.append(self._process_single_batch(batch.copy()))
            gc.collect()
            
        print("ğŸ”— åˆä½µæ‰€æœ‰æ‰¹æ¬¡çµæœ...")
        final_df = pd.concat(processed_batches, ignore_index=True)
        print(f"âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆï¼Œå…± {len(final_df):,} ç­†æœ‰æ•ˆè¨˜éŒ„ã€‚")
        return self.optimize_datatypes(final_df)

    @staticmethod
    def optimize_datatypes(df):
        """æœ€ä½³åŒ–è³‡æ–™å‹åˆ¥ä»¥ç¯€çœè¨˜æ†¶é«”ã€‚"""
        print("ğŸ”§ æœ€ä½³åŒ–è³‡æ–™å‹åˆ¥...")
        initial_memory = df.memory_usage(deep=True).sum() / 1024**2
        
        for col in df.select_dtypes(include=['int64']).columns:
            df[col] = pd.to_numeric(df[col], downcast='integer')
        for col in df.select_dtypes(include=['float64']).columns:
            df[col] = pd.to_numeric(df[col], downcast='float')
            
        final_memory = df.memory_usage(deep=True).sum() / 1024**2
        print(f" ğŸ’¾ è¨˜æ†¶é«”æœ€ä½³åŒ–: {initial_memory:.1f} MB â†’ {final_memory:.1f} MB (ç¯€çœ {((initial_memory - final_memory) / initial_memory * 100):.1f}%)")
        return df

