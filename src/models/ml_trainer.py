# src/models/ml_trainer.py

import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error
from cuml.ensemble import RandomForestRegressor as cuRF
import joblib
import warnings

warnings.filterwarnings('ignore')

class A100OptimizedTrainer:
    """
    åœ¨ NVIDIA A100 GPU ä¸Šé€²è¡Œæœ€ä½³åŒ–è¨“ç·´çš„è¨“ç·´å™¨ã€‚
    - æ”¯æ´ cuML åŠ é€Ÿçš„éš¨æ©Ÿæ£®æ—ã€‚
    - æ”¯æ´ TensorFlow æ··åˆç²¾åº¦è¨“ç·´çš„ç¥ç¶“ç¶²è·¯ã€‚
    - è‡ªå‹•è™•ç†è³‡æ–™åˆ†å‰²ã€æ¨™æº–åŒ–å’Œæ¨¡å‹è©•ä¼°ã€‚
    """
    def __init__(self, use_gpu=True, sample_size=5_000_000):
        self.use_gpu = use_gpu
        self.sample_size = sample_size
        self.scaler = StandardScaler()
        self.rf_model = None
        self.nn_model = None
        self.feature_names = None
        
        # åµæ¸¬ cuML å¯ç”¨æ€§
        try:
            import cuml
            self.cuml_available = True
            print("âœ… cuML å·²æ‰¾åˆ°ï¼Œå°‡ä½¿ç”¨ GPU é€²è¡Œéš¨æ©Ÿæ£®æ—è¨“ç·´ã€‚")
        except ImportError:
            self.cuml_available = False
            print("âš ï¸ cuML æœªæ‰¾åˆ°ï¼Œéš¨æ©Ÿæ£®æ—å°‡ä½¿ç”¨ CPUã€‚")

    def prepare_data(self, df, features, target):
        """æº–å‚™è¨“ç·´è³‡æ–™ï¼ŒåŒ…æ‹¬æ¡æ¨£ã€åˆ†å‰²å’Œæ¨™æº–åŒ–ã€‚"""
        self.feature_names = features
        print("ğŸ“Š æº–å‚™è¨“ç·´è³‡æ–™...")
        
        if len(df) > self.sample_size:
            print(f"å¤§å‹è³‡æ–™é›†ï¼Œæ¡æ¨£ {self.sample_size:,} ç­†è¨˜éŒ„é€²è¡Œè¨“ç·´...")
            df = df.sample(n=self.sample_size, random_state=42)
            
        X = df[features].astype(np.float32)
        y = df[target].astype(np.float32)
        
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        print(f"âœ… è³‡æ–™æº–å‚™å®Œæˆã€‚è¨“ç·´é›†: {X_train.shape}, æ¸¬è©¦é›†: {X_test.shape}")
        return X_train_scaled, X_test_scaled, y_train, y_test

    def train_random_forest(self, X_train, y_train):
        """è¨“ç·´éš¨æ©Ÿæ£®æ—æ¨¡å‹ï¼Œå„ªå…ˆä½¿ç”¨ GPUã€‚"""
        print("\nğŸŒ² è¨“ç·´éš¨æ©Ÿæ£®æ—...")
        if self.use_gpu and self.cuml_available:
            self.rf_model = cuRF(n_estimators=300, max_depth=16, random_state=42, max_features=0.5)
        else:
            self.rf_model = RandomForestRegressor(n_estimators=200, max_depth=12, random_state=42, n_jobs=-1, verbose=1)
        
        self.rf_model.fit(X_train, y_train)
        print("âœ… éš¨æ©Ÿæ£®æ—è¨“ç·´å®Œæˆã€‚")

    def train_neural_network(self, X_train, y_train, X_test, y_test):
        """è¨“ç·´ç¥ç¶“ç¶²è·¯æ¨¡å‹ï¼Œä½¿ç”¨ TensorFlowã€‚"""
        print("\nğŸ§  è¨“ç·´ç¥ç¶“ç¶²è·¯...")
        if self.use_gpu:
            tf.keras.mixed_precision.set_global_policy('mixed_float16')
            print("å·²å•Ÿç”¨æ··åˆç²¾åº¦è¨“ç·´ã€‚")

        self.nn_model = tf.keras.Sequential([
            tf.keras.layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.BatchNormalization(),
            tf.keras.layers.Dropout(0.3),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dense(1, dtype='float32') # è¼¸å‡ºå±¤ä½¿ç”¨ float32
        ])
        
        self.nn_model.compile(optimizer='adam', loss='mse', metrics=['mae'])
        
        callbacks = [
            tf.keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),
            tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5)
        ]
        
        history = self.nn_model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=100,
            batch_size=4096,
            callbacks=callbacks,
            verbose=1
        )
        print("âœ… ç¥ç¶“ç¶²è·¯è¨“ç·´å®Œæˆã€‚")
        return history

    def evaluate_models(self, X_test, y_test):
        """è©•ä¼°å·²è¨“ç·´æ¨¡å‹çš„æ•ˆèƒ½ã€‚"""
        results = {}
        if self.rf_model:
            preds = self.rf_model.predict(X_test)
            results['Random Forest'] = {'R2': r2_score(y_test, preds), 'MAE': mean_absolute_error(y_test, preds)}
        
        if self.nn_model:
            preds = self.nn_model.predict(X_test).flatten()
            results['Neural Network'] = {'R2': r2_score(y_test, preds), 'MAE': mean_absolute_error(y_test, preds)}
        
        print("\nğŸ“ˆ æ¨¡å‹è©•ä¼°çµæœ:")
        for model, metrics in results.items():
            print(f"  - {model}: RÂ²={metrics['R2']:.6f}, MAE={metrics['MAE']:.6f}")
        return results

    def save_models(self, path='.'):
        """å„²å­˜æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨ã€‚"""
        if self.rf_model:
            joblib.dump(self.rf_model, f'{path}/rf_model.pkl')
        if self.nn_model:
            self.nn_model.save(f'{path}/nn_model.h5')
        if self.scaler:
            joblib.dump(self.scaler, f'{path}/scaler.pkl')
        print(f"âœ… æ¨¡å‹å·²å„²å­˜è‡³ '{path}' ç›®éŒ„ã€‚")

